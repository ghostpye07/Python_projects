{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E19CSE410_AIandML_Week-5_Part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMHjtVPbyaKP"
      },
      "source": [
        "## Logistic Regression Model for Divorce Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kktr-4GPI5ou"
      },
      "source": [
        "## Part 1.1: Implement  logistic regression from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJi26z8awmSD"
      },
      "source": [
        "### Logistic regression\n",
        "Logistic regression uses an equation as the representation, very much like linear regression.\n",
        "\n",
        "Input values (x) are combined linearly using weights or coefficient values (referred to as W) to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a continuous value.<br>\n",
        "\n",
        "###  $\\hat{y}(w, x) = \\frac{1}{1+exp^{-(w_0 + w_1 * x_1 + ... + w_p * x_p)}}$\n",
        "\n",
        "#### Dataset\n",
        "The dataset is available at <strong>\"data/divorce.csv\"</strong> in the respective challenge's repo.<br>\n",
        "<strong>Original Source:</strong> https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set. Dataset is based on rating for questionnaire filled by people who already got divorse and those who is happily married.<br><br>\n",
        "\n",
        "[//]: # \"The dataset is available at http://archive.ics.uci.edu/ml/machine-learning-databases/00520/data.zip. Unzip the file and use either CSV or xlsx file.<br>\"\n",
        "\n",
        "\n",
        "#### Features (X)\n",
        "1. Atr1 - If one of us apologizes when our discussion deteriorates, the discussion ends. (Numeric | Range: 0-4)\n",
        "2. Atr2 - I know we can ignore our differences, even if things get hard sometimes. (Numeric | Range: 0-4)\n",
        "3. Atr3 - When we need it, we can take our discussions with my spouse from the beginning and correct it. (Numeric | Range: 0-4)\n",
        "4. Atr4 - When I discuss with my spouse, to contact him will eventually work. (Numeric | Range: 0-4)\n",
        "5. Atr5 - The time I spent with my wife is special for us. (Numeric | Range: 0-4)\n",
        "6. Atr6 - We don't have time at home as partners. (Numeric | Range: 0-4)\n",
        "7. Atr7 - We are like two strangers who share the same environment at home rather than family. (Numeric | Range: 0-4)\n",
        "\n",
        "&emsp;.<br>\n",
        "&emsp;.<br>\n",
        "&emsp;.<br>\n",
        "<br>\n",
        "54. Atr54 - I'm not afraid to tell my spouse about her/his incompetence. (Numeric | Range: 0-4)\n",
        "<br><br>\n",
        "Take a look above at the source of the original dataset for more details.\n",
        "\n",
        "#### Target (y)\n",
        "55. Class: (Binary | 1 => Divorced, 0 => Not divorced yet)\n",
        "\n",
        "#### Objective\n",
        "To gain understanding of logistic regression through implementing the model from scratch\n",
        "\n",
        "#### Tasks\n",
        "- Download and load the data (csv file contains ';' as delimiter)\n",
        "- Add column at position 0 with all values=1 (pandas.DataFrame.insert function). This is for input to the bias $w_0$\n",
        "- Define X matrix (independent features) and y vector (target feature) as numpy arrays\n",
        "- Print the shape and datatype of both X and y\n",
        "[//]: # \"- Dataset contains missing values, hence fill the missing values (NA) by performing missing value prediction\"\n",
        "[//]: # \"- Since the all the features are in higher range, columns can be normalized into smaller scale (like 0 to 1) using different methods such as scaling, standardizing or any other suitable preprocessing technique (sklearn.preprocessing.StandardScaler)\"\n",
        "- Split the dataset into 85% for training and rest 15% for testing (sklearn.model_selection.train_test_split function)\n",
        "- Follow logistic regression class and fill code where highlighted:\n",
        "    - Write sigmoid function to predict probabilities\n",
        "    - Write cross entropy or log loss function (i.e. negative log likelihood)\n",
        "    - Write fit function where gradient descent is implemented\n",
        "    - Write predict_proba function where we predict probabilities for input data\n",
        "- Train the model\n",
        "- Write function for calculating accuracy\n",
        "- Compute accuracy on train and test data\n",
        "\n",
        "#### Further Fun (will not be evaluated)\n",
        "- Play with learning rate and max_iterations\n",
        "- Preprocess data with different feature scaling methods (i.e. scaling, normalization, standardization, etc) and observe accuracies on both X_train and X_test\n",
        "- Train model on different train-test splits such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc. and observe accuracies on both X_train and X_test\n",
        "- Shuffle training samples with different random seed values in the train_test_split function. Check the model error for the testing data for each setup.\n",
        "- Print other classification metrics such as:\n",
        "    - classification report (sklearn.metrics.classification_report),\n",
        "    - confusion matrix (sklearn.metrics.confusion_matrix),\n",
        "    - precision, recall and f1 scores (sklearn.metrics.precision_recall_fscore_support)\n",
        "\n",
        "#### Helpful links\n",
        "- How Logistic Regression works: https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
        "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "- Training testing splitting: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21J6cpd_wmSE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.metrics import accuracy_score\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SL1fdNt1k3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a249f0-b6c8-4b37-8859-79391236d480"
      },
      "source": [
        "# Download the dataset from the source\n",
        "!wget \"https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-20 15:27:15--  https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘Divorce+Predictors+data+set’\n",
            "\n",
            "\r          Divorce+P     [<=>                 ]       0  --.-KB/s               \rDivorce+Predictors+     [ <=>                ]  11.96K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-09-20 15:27:15 (78.3 MB/s) - ‘Divorce+Predictors+data+set’ saved [12243]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9av7W-wowmSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b900d3fb-727a-496d-82c0-df95ee9e7f49"
      },
      "source": [
        "# Read the data from local cloud directory\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/AIandML_Week-5_Part-1/divorce.csv\",delimiter=\";\")\n",
        "type(data)\n",
        "# Set delimiter to semicolon(;) in case of unexpected results"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmXolHwEI5o8"
      },
      "source": [
        "# Add column which has all 1s\n",
        "if \"Weights\" not in data.columns:\n",
        "  data.insert(0, 'Weights', 1)\n",
        "# The idea is that weight corresponding to this column is equal to intercept\n",
        "# This way it is efficient and easier to handle the bias/intercept term\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV1jGAQxwmSP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "6cc955e1-e9be-4c04-d676-231af41365c4"
      },
      "source": [
        "# Print the dataframe rows just to see some samples\n",
        "data.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weights</th>\n",
              "      <th>Atr1</th>\n",
              "      <th>Atr2</th>\n",
              "      <th>Atr3</th>\n",
              "      <th>Atr4</th>\n",
              "      <th>Atr5</th>\n",
              "      <th>Atr6</th>\n",
              "      <th>Atr7</th>\n",
              "      <th>Atr8</th>\n",
              "      <th>Atr9</th>\n",
              "      <th>Atr10</th>\n",
              "      <th>Atr11</th>\n",
              "      <th>Atr12</th>\n",
              "      <th>Atr13</th>\n",
              "      <th>Atr14</th>\n",
              "      <th>Atr15</th>\n",
              "      <th>Atr16</th>\n",
              "      <th>Atr17</th>\n",
              "      <th>Atr18</th>\n",
              "      <th>Atr19</th>\n",
              "      <th>Atr20</th>\n",
              "      <th>Atr21</th>\n",
              "      <th>Atr22</th>\n",
              "      <th>Atr23</th>\n",
              "      <th>Atr24</th>\n",
              "      <th>Atr25</th>\n",
              "      <th>Atr26</th>\n",
              "      <th>Atr27</th>\n",
              "      <th>Atr28</th>\n",
              "      <th>Atr29</th>\n",
              "      <th>Atr30</th>\n",
              "      <th>Atr31</th>\n",
              "      <th>Atr32</th>\n",
              "      <th>Atr33</th>\n",
              "      <th>Atr34</th>\n",
              "      <th>Atr35</th>\n",
              "      <th>Atr36</th>\n",
              "      <th>Atr37</th>\n",
              "      <th>Atr38</th>\n",
              "      <th>Atr39</th>\n",
              "      <th>Atr40</th>\n",
              "      <th>Atr41</th>\n",
              "      <th>Atr42</th>\n",
              "      <th>Atr43</th>\n",
              "      <th>Atr44</th>\n",
              "      <th>Atr45</th>\n",
              "      <th>Atr46</th>\n",
              "      <th>Atr47</th>\n",
              "      <th>Atr48</th>\n",
              "      <th>Atr49</th>\n",
              "      <th>Atr50</th>\n",
              "      <th>Atr51</th>\n",
              "      <th>Atr52</th>\n",
              "      <th>Atr53</th>\n",
              "      <th>Atr54</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Weights  Atr1  Atr2  Atr3  Atr4  ...  Atr51  Atr52  Atr53  Atr54  Class\n",
              "0        1     2     2     4     1  ...      2      3      2      1      1\n",
              "1        1     4     4     4     4  ...      4      4      2      2      1\n",
              "2        1     2     2     2     2  ...      1      2      2      2      1\n",
              "3        1     3     2     3     2  ...      3      2      2      2      1\n",
              "4        1     2     2     1     1  ...      2      2      1      0      1\n",
              "5        1     0     0     1     0  ...      1      1      2      0      1\n",
              "6        1     3     3     3     2  ...      3      2      2      2      1\n",
              "7        1     2     1     2     2  ...      1      1      1      0      1\n",
              "8        1     2     2     1     0  ...      1      1      1      1      1\n",
              "9        1     1     1     1     1  ...      2      4      3      3      1\n",
              "\n",
              "[10 rows x 56 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joRU6dWxwmSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa91dea1-a376-4e5f-af35-f321a6acb122"
      },
      "source": [
        "# Define X (input features) and y (output feature) \n",
        "X = np.array(data.iloc[:,:55])\n",
        "y = np.array(data.iloc[:,55:])\n",
        "y"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAyM-CYCwmSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc601c9-dc33-432e-d84b-8e866dd1a95d"
      },
      "source": [
        "X_shape = X.shape\n",
        "X_type  = type(X)\n",
        "y_shape = y.shape \n",
        "y_type  = type(y)\n",
        "print(f'X: Type-{X_type}, Shape-{X_shape}')\n",
        "print(f'y: Type-{y_type}, Shape-{y_shape}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: Type-<class 'numpy.ndarray'>, Shape-(170, 55)\n",
            "y: Type-<class 'numpy.ndarray'>, Shape-(170, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJrEelFzI5pA"
      },
      "source": [
        "<strong>Expected output: </strong><br><br>\n",
        "\n",
        "X: Type-<class 'numpy.ndarray'>, Shape-(170, 55)<br>\n",
        "y: Type-<class 'numpy.ndarray'>, Shape-(170,)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdLIVOm127-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7b7fd1-98bb-43f3-9fe5-d159a9f4dea2"
      },
      "source": [
        "# Check and fill any missing values if any\n",
        "imp = SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
        "data=imp.fit_transform(data)\n",
        "data_frame=pd.DataFrame(data)\n",
        "type(data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En9Kb9dh2-wm"
      },
      "source": [
        "# Perform standarization (if required)\n",
        "sc=StandardScaler()\n",
        "\n",
        "data1 = sc.fit_transform(data)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8WF-EqO3BEa"
      },
      "source": [
        "# Split the dataset into training and testing here\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=41)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acCATJhI3FdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ee1f3d-07cd-409a-e185-b996605965c2"
      },
      "source": [
        "# Print the shape of features and target of training and testing: X_train, X_test, y_train, y_test\n",
        "X_train_shape = X_train.shape\n",
        "y_train_shape = y_train.shape\n",
        "X_test_shape  = X_test.shape\n",
        "y_test_shape  = y_test.shape\n",
        "\n",
        "print(f\"X_train: {X_train_shape} , y_train: {y_train_shape}\")\n",
        "print(f\"X_test: {X_test_shape} , y_test: {y_test_shape}\")\n",
        "assert (X_train.shape[0]==y_train.shape[0] and X_test.shape[0]==y_test.shape[0]), \"Check your splitting carefully\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (144, 55) , y_train: (144, 1)\n",
            "X_test: (26, 55) , y_test: (26, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSa7cW-NwmSd"
      },
      "source": [
        "##### Let us start implementing logistic regression from scratch. Just follow code cells, see hints if required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzxCdzxBI5pF"
      },
      "source": [
        "##### We will build a LogisticRegression class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQgpspPcI5pH"
      },
      "source": [
        "# DO NOT EDIT ANY VARIABLE OR FUNCTION NAME(S) IN THIS CELL\n",
        "# Let's try more object oriented approach this time :)\n",
        "class MyLogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
        "        '''Initialize variables\n",
        "        Args:\n",
        "            learning_rate  : Learning Rate\n",
        "            max_iterations : Max iterations for training weights\n",
        "        '''\n",
        "        # Initialising all the parameters\n",
        "        self.learning_rate  = learning_rate\n",
        "        self.max_iterations = max_iterations\n",
        "        self.cross_entropy_error    = [] # Summary of the cross entroy or log loss (i.e. negative log-likelihood)\n",
        "        \n",
        "        # Define epsilon because log(0) is not defined\n",
        "        self.eps = 1e-7\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        '''Sigmoid function: f:R->(0,1)\n",
        "        Args:\n",
        "            z : A numpy array (num_samples,)\n",
        "        Returns:\n",
        "            A numpy array where sigmoid function applied to every element\n",
        "        '''\n",
        "        ### START CODE HERE\n",
        "        sig_z = 1/(1+np.exp(-1 * z))\n",
        "        ### END CODE HERE\n",
        "        assert (z.shape==sig_z.shape), 'Error in sigmoid implementation. Check carefully'\n",
        "        return sig_z\n",
        "    \n",
        "    def cross_entropy(self, y_true, y_pred):\n",
        "        '''Calculates cross_entropy or log loss (negative log-likelihood) estimate\n",
        "        Remember: -[y * log(yh) + (1-y) * log(1-yh)]\n",
        "        Note: Cross_entropy or log-loss is defined for multiple classes as well, but for this dataset\n",
        "        \n",
        "        Args:\n",
        "            y_true : Numpy array of actual truth values (num_samples,)\n",
        "            y_pred : Numpy array of predicted values (num_samples,)\n",
        "        Returns:\n",
        "            cross_entropy(or log loss which is the negative Log-likelihood) scalar value\n",
        "        '''\n",
        "        ### START CODE HERE\n",
        "        ind_logloss = np.nan_to_num(np.multiply(-1,[y_true * np.log(y_pred, out=np.nan_to_num(y_pred,math.exp(-7)), where=(y_pred!=0) ) \n",
        "                      + (np.subtract(1,y_true)) * np.log(1-y_pred, out=np.nan_to_num(1-y_pred,math.exp(-7)), where=(1-y_pred!=0) )]),nan=math.exp(-7))\n",
        "                    \n",
        "        cross_entropy = ind_logloss.sum() \n",
        "        ### END CODE HERE\n",
        "        \n",
        "        return cross_entropy\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        '''Trains logistic regression model using gradient descent\n",
        "        to gain minimum cross_entropy/log-loss on the training data\n",
        "        Args:\n",
        "            X : Numpy array (num_examples, num_features)\n",
        "            y : Numpy array (num_examples, )\n",
        "        Returns: VOID\n",
        "        '''\n",
        "        \n",
        "        num_examples = X.shape[0]\n",
        "        num_features = X.shape[1]\n",
        "        \n",
        "        ### START CODE HERE\n",
        "        # Initialize weights with appropriate shape\n",
        "        self.weights = np.zeros(num_features).reshape(-1,1) \n",
        "        # Perform gradient descent\n",
        "        for i in range(self.max_iterations):\n",
        "            # Define the linear hypothesis(z) first\n",
        "            # HINT: what is our hypothesis function in linear regression, remember?\n",
        "            z = np.dot(X,self.weights)\n",
        "            # Output probability value by appplying sigmoid on z\n",
        "            y_pred = self.sigmoid(z)\n",
        "            \n",
        "            # Calculate the gradient values\n",
        "            # This is just vectorized efficient way of implementing gradient. Don't worry, we will discuss it later.\n",
        "            # print(((y_pred-y)*X).shape)\n",
        "            \n",
        "            gradient = np.mean((y_pred-y)*X)\n",
        "            \n",
        "            # Update the weights using gradient descent\n",
        "            self.weights = self.weights - (self.learning_rate) * (gradient) \n",
        "            \n",
        "            # Calculating cross entropy or log-loss (negatie log likelihood)\n",
        "            cross_entropy = self.cross_entropy(y, y_pred)\n",
        "\n",
        "            self.cross_entropy_error.append(cross_entropy)\n",
        "    \n",
        "        # ### END CODE HERE\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        '''Predict probabilities for given X.\n",
        "        Remember sigmoid returns value between 0 and 1.\n",
        "        Args:\n",
        "            X : Numpy array (num_samples, num_features)\n",
        "        Returns:\n",
        "            probabilities: Numpy array (num_samples,)\n",
        "        '''\n",
        "        if self.weights is None:\n",
        "            raise Exception(\"Fit the model before prediction\")\n",
        "        \n",
        "        ### START CODE HERE\n",
        "        z = np.dot(X,self.weights)\n",
        "        probabilities = self.sigmoid(z)\n",
        "        ### END CODE HERE\n",
        "        \n",
        "        return probabilities\n",
        "    \n",
        "    def predict(self, X, threshold=0.60):\n",
        "        '''Predict/Classify X in classes\n",
        "        Args:\n",
        "            X         : Numpy array (num_samples, num_features)\n",
        "            threshold : scalar value above which prediction is 1 else 0\n",
        "        Returns:\n",
        "            binary_predictions : Numpy array (num_samples,)\n",
        "        '''\n",
        "        # Thresholding probability to predict binary values\n",
        "        binary_predictions = np.array(list(map(lambda x: 1 if x>threshold else 0, self.predict_proba(X))))\n",
        "        \n",
        "        return binary_predictions"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crl6mGEoI5pJ"
      },
      "source": [
        "# Now initialize logitic regression implemented by you\n",
        "model = MyLogisticRegression()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmMnrzPBI5pK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb151077-382c-4e62-9e1b-5ba41857fe21"
      },
      "source": [
        "# And now fit on training data\n",
        "model.fit(X_train,y_train)\n",
        "model.predict(X)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK6tFOQTI5pL"
      },
      "source": [
        "##### Phew!! That's a lot of code. But you did it, congrats !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tvMc0OqwmSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d36422-3dae-4f0d-e0f9-ef490e628714"
      },
      "source": [
        "# Training cross entropy cost (or log-loss)\n",
        "train_cross_entropy = model.cross_entropy(y_train, model.predict_proba(X_train))\n",
        "print(\"Cross entropy cost on training data:\", train_cross_entropy)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross entropy cost on training data: 75.79522550385123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGViZYRDLcIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e96fec43-c689-4100-e56a-eb1861f94da9"
      },
      "source": [
        "# Testing cross entropy cost (or log-loss)\n",
        "test_cross_entropy = model.cross_entropy(y_test, model.predict_proba(X_test))\n",
        "print(\"Cross entropy cost on testing data:\", test_cross_entropy)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross entropy cost on testing data: 12.796403206061807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vnjkAvzI5pN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4ac1f6e2-03d0-42b2-b8f2-0aa41fbc45c6"
      },
      "source": [
        "# Plot the loss curve\n",
        "plt.plot([i+1 for i in range(len(model.cross_entropy_error))], model.cross_entropy_error)\n",
        "plt.title(\"Cross entropy error curve\")\n",
        "plt.xlabel(\"Iteration num\")\n",
        "plt.ylabel(\"Cross entropy (-ve log-likelihood)\")\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH-ocni8I5pO"
      },
      "source": [
        "##### Let's calculate accuracy as well. Accuracy is defined simply as the rate of correct classifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-RxIPbVI5pP"
      },
      "source": [
        "#Make predictions on test data\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsS3PX65I5pP"
      },
      "source": [
        "def accuracy(y_true,y_pred):\n",
        "    '''Compute accuracy.\n",
        "    Accuracy = (Correct prediction / number of samples)\n",
        "    Args:\n",
        "        y_true : Truth binary values (num_examples, )\n",
        "        y_pred : Predicted binary values (num_examples, )\n",
        "    Returns:\n",
        "        accuracy: scalar value\n",
        "    '''\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    \n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    ### END CODE HERE\n",
        "    return accuracy"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05SA_Ur6I5pQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67df79b-c00b-4077-c33f-e411a5b2a021"
      },
      "source": [
        "# Print accuracy on train data\n",
        "acc = accuracy(y_train, model.predict(X_train))\n",
        "acc"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7986111111111112"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ0zLpChI5pQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632c0796-f07a-4919-cf59-f5bdd6ac83a8"
      },
      "source": [
        "# Print accuracy on test data\n",
        "acc = accuracy(y_test, model.predict(X_test))\n",
        "acc"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8076923076923077"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPU37tWAI5pR"
      },
      "source": [
        "## Part 1.2: Use Logistic Regression from sklearn on the same dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHscCs3bI5pR"
      },
      "source": [
        "#### Tasks\n",
        "- Define X and y again for sklearn Linear Regression model\n",
        "- Train Logistic Regression Model on the training set (sklearn.linear_model.LogisticRegression class)\n",
        "- Run the model on testing set\n",
        "- Print 'accuracy' obtained on the testing dataset (sklearn.metrics.accuracy_score function)\n",
        "\n",
        "#### Further fun (will not be evaluated)\n",
        "- Compare accuracies of your model and sklearn's logistic regression model\n",
        "\n",
        "#### Helpful links\n",
        "- Classification metrics in sklearn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcM8FakEI5pT"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP8AhS_vI5pT"
      },
      "source": [
        "# Define X and y\n",
        "X = np.array(data[:,:55])\n",
        "y = np.array(data[:,55:])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc7htjejI5pU"
      },
      "source": [
        "# Initialize the model from sklearn\n",
        "model = LogisticRegression(random_state=0)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owIrjceyI5pU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201e6829-79f3-4db3-a8fa-4561ea86b364"
      },
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_YS9dn8I5pV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e5f1339-5a34-43b2-aff6-c970aa46b4ee"
      },
      "source": [
        "# Predict on testing set X_test\n",
        "y_pred = model.predict(X)\n",
        "y_pred"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pBqDQ0qI5pW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa88b4db-2601-4ae3-9040-fc4b1e05a1c2"
      },
      "source": [
        "# Print Accuracy on testing set\n",
        "test_accuracy_sklearn = accuracy(y, y_pred)\n",
        "\n",
        "print(f\"\\nAccuracy on testing set: {test_accuracy_sklearn}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy on testing set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zONyMZ7ToCy3",
        "outputId": "4bd0c363-a6fd-4323-f62e-b378c389f152"
      },
      "source": [
        "model."
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}